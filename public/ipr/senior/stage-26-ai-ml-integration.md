# –≠—Ç–∞–ø 26: AI/ML Integration - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è

## –ó–∞–¥–∞–Ω–∏–µ 26.1: –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏

### üéØ –¶–µ–ª—å

–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–Ω, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏

### üìã –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

#### –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:

- [ ] –ú–æ–¥–µ–ª—å –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–Ω –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
- [ ] –°–∏—Å—Ç–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
- [ ] –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- [ ] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ ML –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã
- [ ] A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π

#### ML Pipeline –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:

- [ ] **–°–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –∏–Ω–∂–µ–Ω–µ—Ä–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- [ ] **–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π**: –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è
- [ ] **–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ**: –ë–µ—Å—à–æ–≤–Ω–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –≤ –ø—Ä–æ–¥–∞–∫—à–Ω
- [ ] **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π
- [ ] **–í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ**: –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–µ—Ä—Å–∏—è–º–∏ –º–æ–¥–µ–ª–µ–π –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤

### üí° –ü–æ–¥—Å–∫–∞–∑–∫–∏

**ML Pipeline —Å MLflow –∏ Apache Airflow:**

```python
# ml-platform/src/pipelines/crypto_price_prediction.py
import mlflow
import mlflow.sklearn
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

class CryptoPricePredictionPipeline:
    def __init__(self):
        self.experiment_name = "crypto_price_prediction"
        self.models = {
            'random_forest': RandomForestRegressor(random_state=42),
            'gradient_boosting': GradientBoostingRegressor(random_state=42),
        }
        self.scaler = StandardScaler()

    def setup_mlflow(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ MLflow —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
        mlflow.set_tracking_uri("http://localhost:5000")
        mlflow.set_experiment(self.experiment_name)

    def extract_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """–ò–Ω–∂–µ–Ω–µ—Ä–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–Ω"""
        features = data.copy()

        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        features['sma_7'] = features['price'].rolling(window=7).mean()
        features['sma_30'] = features['price'].rolling(window=30).mean()
        features['ema_12'] = features['price'].ewm(span=12).mean()
        features['ema_26'] = features['price'].ewm(span=26).mean()

        # MACD
        features['macd'] = features['ema_12'] - features['ema_26']
        features['macd_signal'] = features['macd'].ewm(span=9).mean()
        features['macd_histogram'] = features['macd'] - features['macd_signal']

        # RSI
        delta = features['price'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        features['rsi'] = 100 - (100 / (1 + rs))

        # Bollinger Bands
        features['bb_middle'] = features['price'].rolling(window=20).mean()
        bb_std = features['price'].rolling(window=20).std()
        features['bb_upper'] = features['bb_middle'] + (bb_std * 2)
        features['bb_lower'] = features['bb_middle'] - (bb_std * 2)
        features['bb_width'] = features['bb_upper'] - features['bb_lower']
        features['bb_position'] = (features['price'] - features['bb_lower']) / features['bb_width']

        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        features['volatility_7d'] = features['price'].rolling(window=7).std()
        features['volatility_30d'] = features['price'].rolling(window=30).std()

        # –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        features['volume_sma_7'] = features['volume'].rolling(window=7).mean()
        features['volume_ratio'] = features['volume'] / features['volume_sma_7']

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['hour'] = pd.to_datetime(features['timestamp']).dt.hour
        features['day_of_week'] = pd.to_datetime(features['timestamp']).dt.dayofweek
        features['month'] = pd.to_datetime(features['timestamp']).dt.month

        # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for lag in [1, 3, 7, 24]:
            features[f'price_lag_{lag}'] = features['price'].shift(lag)
            features[f'volume_lag_{lag}'] = features['volume'].shift(lag)

        # –û—Ç–Ω–æ—à–µ–Ω–∏—è —Ü–µ–Ω
        features['price_change_1h'] = features['price'].pct_change(1)
        features['price_change_24h'] = features['price'].pct_change(24)
        features['price_change_7d'] = features['price'].pct_change(24*7)

        # –£–¥–∞–ª–µ–Ω–∏–µ NaN –∑–Ω–∞—á–µ–Ω–∏–π
        features = features.dropna()

        return features

    def prepare_target(self, data: pd.DataFrame, prediction_horizon: int = 1) -> pd.Series:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è"""
        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º —Ü–µ–Ω—É —á–µ—Ä–µ–∑ prediction_horizon —á–∞—Å–æ–≤
        target = data['price'].shift(-prediction_horizon)
        return target.dropna()

    def train_model(self, X_train: pd.DataFrame, y_train: pd.Series, model_name: str):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å MLflow tracking"""
        with mlflow.start_run(run_name=f"{model_name}_training"):
            model = self.models[model_name]

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            mlflow.log_params(model.get_params())

            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model.fit(X_train, y_train)

            # –ü—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ
            y_pred_train = model.predict(X_train)

            # –ú–µ—Ç—Ä–∏–∫–∏
            train_mae = mean_absolute_error(y_train, y_pred_train)
            train_mse = mean_squared_error(y_train, y_pred_train)
            train_r2 = r2_score(y_train, y_pred_train)

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            mlflow.log_metrics({
                'train_mae': train_mae,
                'train_mse': train_mse,
                'train_r2': train_r2,
            })

            # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if hasattr(model, 'feature_importances_'):
                feature_importance = pd.DataFrame({
                    'feature': X_train.columns,
                    'importance': model.feature_importances_
                }).sort_values('importance', ascending=False)

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç
                feature_importance.to_csv('feature_importance.csv', index=False)
                mlflow.log_artifact('feature_importance.csv')

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            mlflow.sklearn.log_model(model, f"{model_name}_model")

            return model

    def hyperparameter_tuning(self, X_train: pd.DataFrame, y_train: pd.Series, model_name: str):
        """–ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é GridSearchCV"""
        param_grids = {
            'random_forest': {
                'n_estimators': [100, 200, 300],
                'max_depth': [10, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            },
            'gradient_boosting': {
                'n_estimators': [100, 200, 300],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7],
                'subsample': [0.8, 0.9, 1.0]
            }
        }

        with mlflow.start_run(run_name=f"{model_name}_hyperparameter_tuning"):
            model = self.models[model_name]
            param_grid = param_grids[model_name]

            # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            tscv = TimeSeriesSplit(n_splits=5)

            # Grid Search
            grid_search = GridSearchCV(
                model, param_grid, cv=tscv,
                scoring='neg_mean_absolute_error',
                n_jobs=-1, verbose=1
            )

            grid_search.fit(X_train, y_train)

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            mlflow.log_params(grid_search.best_params_)
            mlflow.log_metric('best_cv_score', -grid_search.best_score_)

            return grid_search.best_estimator_

    def validate_model(self, model, X_test: pd.DataFrame, y_test: pd.Series):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        y_pred = model.predict(X_test)

        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        # –†–∞—Å—á–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã
        direction_accuracy = np.mean(
            (np.sign(y_pred - X_test['price_lag_1'].values) ==
             np.sign(y_test - X_test['price_lag_1'].values))
        )

        metrics = {
            'test_mae': mae,
            'test_mse': mse,
            'test_r2': r2,
            'direction_accuracy': direction_accuracy
        }

        return metrics, y_pred

    def run_pipeline(self, data_path: str, symbol: str):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ ML pipeline"""
        self.setup_mlflow()

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        data = pd.read_csv(data_path)
        data = data[data['symbol'] == symbol].copy()

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features = self.extract_features(data)
        target = self.prepare_target(data, prediction_horizon=1)

        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        min_len = min(len(features), len(target))
        X = features.iloc[:min_len]
        y = target.iloc[:min_len]

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏ (80/20)
        split_idx = int(0.8 * len(X))
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_columns = X_train.select_dtypes(include=[np.number]).columns
        X_train_scaled = X_train.copy()
        X_test_scaled = X_test.copy()

        X_train_scaled[feature_columns] = self.scaler.fit_transform(X_train[feature_columns])
        X_test_scaled[feature_columns] = self.scaler.transform(X_test[feature_columns])

        # –û–±—É—á–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        results = {}
        for model_name in self.models.keys():
            print(f"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name}")

            # –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            best_model = self.hyperparameter_tuning(X_train_scaled, y_train, model_name)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            metrics, predictions = self.validate_model(best_model, X_test_scaled, y_test)
            results[model_name] = {
                'model': best_model,
                'metrics': metrics,
                'predictions': predictions
            }

            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã {model_name}: MAE={metrics['test_mae']:.4f}, "
                  f"R¬≤={metrics['test_r2']:.4f}, "
                  f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è={metrics['direction_accuracy']:.4f}")

        # –í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        best_model_name = min(results.keys(),
                            key=lambda x: results[x]['metrics']['test_mae'])

        print(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
        return results[best_model_name]

# –ó–∞–ø—É—Å–∫ pipeline
if __name__ == "__main__":
    pipeline = CryptoPricePredictionPipeline()
    best_model = pipeline.run_pipeline("crypto_price_data.csv", "BTC")
```

**–°–∏—Å—Ç–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π:**

```typescript
// ml-platform/src/anomaly-detection/FraudDetectionService.ts
import * as tf from "@tensorflow/tfjs-node";
import { z } from "zod";

interface Transaction {
  id: string;
  userId: string;
  amount: number;
  timestamp: number;
  fromAddress: string;
  toAddress: string;
  gasPrice: number;
  metadata: Record<string, any>;
}

export class FraudDetectionService {
  private model: tf.LayersModel | null = null;
  private scaler: { mean: number[]; std: number[] } | null = null;
  private featureNames: string[] = [];

  async loadModel(modelPath: string): Promise<void> {
    this.model = await tf.loadLayersModel(`file://${modelPath}`);
    console.log("–ú–æ–¥–µ–ª—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞");
  }

  extractFeatures(
    transaction: Transaction,
    userHistory: Transaction[]
  ): number[] {
    const features: Record<string, number> = {};

    // –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
    features.amount = transaction.amount;
    features.gas_price = transaction.gasPrice;
    features.hour_of_day = new Date(transaction.timestamp).getHours();
    features.day_of_week = new Date(transaction.timestamp).getDay();

    // –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    if (userHistory.length > 0) {
      const amounts = userHistory.map((t) => t.amount);
      const timestamps = userHistory.map((t) => t.timestamp);

      features.user_avg_amount =
        amounts.reduce((a, b) => a + b, 0) / amounts.length;
      features.user_max_amount = Math.max(...amounts);
      features.user_std_amount = this.standardDeviation(amounts);
      features.user_transaction_frequency =
        userHistory.length /
        Math.max(
          1,
          (Math.max(...timestamps) - Math.min(...timestamps)) /
            (24 * 60 * 60 * 1000)
        );

      // –û—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç –æ–±—ã—á–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è
      features.amount_deviation =
        Math.abs(transaction.amount - features.user_avg_amount) /
        Math.max(1, features.user_std_amount);

      // –í—Ä–µ–º—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
      const lastTransaction = Math.max(...timestamps);
      features.time_since_last =
        (transaction.timestamp - lastTransaction) / (60 * 60 * 1000); // —á–∞—Å—ã
    } else {
      // –ù–æ–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
      features.user_avg_amount = 0;
      features.user_max_amount = 0;
      features.user_std_amount = 0;
      features.user_transaction_frequency = 0;
      features.amount_deviation = 0;
      features.time_since_last = 0;
      features.is_new_user = 1;
    }

    // –°–µ—Ç–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    features.address_reuse = userHistory.filter(
      (t) => t.toAddress === transaction.toAddress
    ).length;

    // –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    const recentTransactions = userHistory.filter(
      (t) => transaction.timestamp - t.timestamp < 24 * 60 * 60 * 1000
    );
    features.transactions_last_24h = recentTransactions.length;
    features.volume_last_24h = recentTransactions.reduce(
      (sum, t) => sum + t.amount,
      0
    );

    // –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –º–∞—Å—Å–∏–≤
    this.featureNames = Object.keys(features);
    return Object.values(features);
  }

  async predict(
    transaction: Transaction,
    userHistory: Transaction[]
  ): Promise<{
    fraud_probability: number;
    risk_level: "low" | "medium" | "high";
    explanation: string[];
  }> {
    if (!this.model) {
      throw new Error("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞");
    }

    const features = this.extractFeatures(transaction, userHistory);

    // –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    const normalizedFeatures = this.normalizeFeatures(features);

    // –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    const input = tf.tensor2d([normalizedFeatures]);
    const prediction = this.model.predict(input) as tf.Tensor;
    const fraudProbability = await prediction.data();

    // –û—á–∏—Å—Ç–∫–∞ —Ç–µ–Ω–∑–æ—Ä–æ–≤
    input.dispose();
    prediction.dispose();

    const probability = fraudProbability[0];

    // –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —Ä–∏—Å–∫–∞
    let riskLevel: "low" | "medium" | "high" = "low";
    if (probability > 0.8) riskLevel = "high";
    else if (probability > 0.5) riskLevel = "medium";

    // –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    const explanation = this.explainPrediction(features, probability);

    return {
      fraud_probability: probability,
      risk_level: riskLevel,
      explanation,
    };
  }

  private normalizeFeatures(features: number[]): number[] {
    if (!this.scaler) {
      // –í –ø—Ä–æ–¥–∞–∫—à–Ω –∑–∞–≥—Ä—É–∂–∞—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
      return features;
    }

    return features.map((value, index) => {
      const mean = this.scaler!.mean[index] || 0;
      const std = this.scaler!.std[index] || 1;
      return (value - mean) / std;
    });
  }

  private explainPrediction(features: number[], probability: number): string[] {
    const explanations: string[] = [];

    // –ê–Ω–∞–ª–∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ —Ä–∏—Å–∫–∞
    if (features[this.featureNames.indexOf("amount_deviation")] > 3) {
      explanations.push(
        "–°—É–º–º–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ–±—ã—á–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"
      );
    }

    if (features[this.featureNames.indexOf("transactions_last_24h")] > 10) {
      explanations.push(
        "–ù–µ–æ–±—ã—á–Ω–æ –≤—ã—Å–æ–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞"
      );
    }

    if (features[this.featureNames.indexOf("is_new_user")] === 1) {
      explanations.push("–ù–æ–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –±–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π");
    }

    const hourOfDay = features[this.featureNames.indexOf("hour_of_day")];
    if (hourOfDay < 6 || hourOfDay > 22) {
      explanations.push("–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏—è –≤ –Ω–µ–æ–±—ã—á–Ω–æ–µ –≤—Ä–µ–º—è —Å—É—Ç–æ–∫");
    }

    if (probability > 0.8) {
      explanations.push("–í—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏");
    } else if (probability > 0.5) {
      explanations.push("–£–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–∏—Å–∫ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞");
    }

    return explanations;
  }

  private standardDeviation(values: number[]): number {
    const mean = values.reduce((a, b) => a + b, 0) / values.length;
    const variance =
      values.reduce((sum, value) => sum + Math.pow(value - mean, 2), 0) /
      values.length;
    return Math.sqrt(variance);
  }

  async batchPredict(transactions: Transaction[]): Promise<Map<string, any>> {
    const results = new Map();

    for (const transaction of transactions) {
      // –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
      const userHistory: Transaction[] = [];

      try {
        const prediction = await this.predict(transaction, userHistory);
        results.set(transaction.id, prediction);
      } catch (error) {
        console.error(
          `–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ ${transaction.id}:`,
          error
        );
        results.set(transaction.id, { error: error.message });
      }
    }

    return results;
  }
}
```

### ‚ùì –í–æ–ø—Ä–æ—Å—ã –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è

1. **Feature Engineering**: –ö–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–Ω –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç?
2. **Model Selection**: –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ vs —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ ML –∞–ª–≥–æ—Ä–∏—Ç–º—ã?
3. **Real-time Inference**: –ö–∞–∫ –æ–±–µ—Å–ø–µ—á–∏—Ç—å –Ω–∏–∑–∫—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤ –ø—Ä–æ–¥–∞–∫—à–Ω?
4. **Model Monitoring**: –ö–∞–∫ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π?

---

## –ó–∞–¥–∞–Ω–∏–µ 26.2: MLOps –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞

### üéØ –¶–µ–ª—å

–ü–æ—Å—Ç—Ä–æ–∏—Ç—å end-to-end MLOps –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ ML –º–æ–¥–µ–ª–µ–π

### üìã –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

#### –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:

- [ ] –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞–π–ø–ª–∞–π–Ω—ã –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
- [ ] –°–∏—Å—Ç–µ–º–∞ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ –¥–∞–Ω–Ω—ã—Ö
- [ ] A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π
- [ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
- [ ] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞

### üí° –ü–æ–¥—Å–∫–∞–∑–∫–∏

**Kubeflow Pipeline –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ ML:**

```python
# mlops/pipelines/training_pipeline.py
import kfp
from kfp import dsl
from kfp.v2 import compiler
from kfp.v2.dsl import component, pipeline, Input, Output, Dataset, Model

@component(
    base_image="python:3.9",
    packages_to_install=["pandas", "scikit-learn", "mlflow"]
)
def data_preprocessing(
    input_data: Input[Dataset],
    processed_data: Output[Dataset],
    feature_store_uri: str
):
    """–ö–æ–º–ø–æ–Ω–µ–Ω—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    import pandas as pd
    import mlflow

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = pd.read_csv(input_data.path)

    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    # ... –ª–æ–≥–∏–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ ...

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    df.to_csv(processed_data.path, index=False)

    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow
    mlflow.log_artifacts(processed_data.path)

@component(
    base_image="python:3.9",
    packages_to_install=["scikit-learn", "mlflow", "joblib"]
)
def model_training(
    training_data: Input[Dataset],
    model_artifact: Output[Model],
    hyperparameters: dict
):
    """–ö–æ–º–ø–æ–Ω–µ–Ω—Ç –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    import pandas as pd
    import mlflow
    import joblib
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_absolute_error

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = pd.read_csv(training_data.path)
    X = df.drop(['target'], axis=1)
    y = df['target']

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = RandomForestRegressor(**hyperparameters)
    model.fit(X, y)

    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    predictions = model.predict(X)
    mae = mean_absolute_error(y, predictions)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    joblib.dump(model, model_artifact.path)

    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow
    with mlflow.start_run():
        mlflow.log_params(hyperparameters)
        mlflow.log_metric("mae", mae)
        mlflow.sklearn.log_model(model, "model")

@component(
    base_image="python:3.9",
    packages_to_install=["scikit-learn", "joblib"]
)
def model_evaluation(
    model_artifact: Input[Model],
    test_data: Input[Dataset],
    evaluation_metrics: Output[Dataset],
    accuracy_threshold: float = 0.8
):
    """–ö–æ–º–ø–æ–Ω–µ–Ω—Ç –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏"""
    import pandas as pd
    import joblib
    from sklearn.metrics import mean_absolute_error, r2_score

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã—Ö
    model = joblib.load(model_artifact.path)
    test_df = pd.read_csv(test_data.path)

    X_test = test_df.drop(['target'], axis=1)
    y_test = test_df['target']

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    predictions = model.predict(X_test)

    # –ú–µ—Ç—Ä–∏–∫–∏
    mae = mean_absolute_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    model_approved = r2 >= accuracy_threshold

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    metrics = {
        'mae': mae,
        'r2': r2,
        'approved': model_approved
    }

    pd.DataFrame([metrics]).to_csv(evaluation_metrics.path, index=False)

@pipeline(
    name="crypto-price-prediction-pipeline",
    description="–ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–Ω –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç"
)
def training_pipeline(
    data_source: str,
    hyperparameters: dict = {"n_estimators": 100, "max_depth": 10},
    accuracy_threshold: float = 0.8
):
    """–û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è"""

    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    preprocess_task = data_preprocessing(
        input_data=data_source,
        feature_store_uri="s3://crypto-feature-store"
    )

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    training_task = model_training(
        training_data=preprocess_task.outputs['processed_data'],
        hyperparameters=hyperparameters
    )

    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    evaluation_task = model_evaluation(
        model_artifact=training_task.outputs['model_artifact'],
        test_data=preprocess_task.outputs['processed_data'],
        accuracy_threshold=accuracy_threshold
    )

    return evaluation_task

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
if __name__ == "__main__":
    compiler.Compiler().compile(
        pipeline_func=training_pipeline,
        package_path="crypto_training_pipeline.yaml"
    )
```

### üîç –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏

- [ ] **ML Pipeline** (40): –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è
- [ ] **Feature Engineering** (30): –ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –∏—Ö –≤–∞–∂–Ω–æ—Å—Ç—å
- [ ] **Model Performance** (20): –¢–æ—á–Ω–æ—Å—Ç—å –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π
- [ ] **MLOps Infrastructure** (10): –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, A/B —Ç–µ—Å—Ç—ã

---

## üìä –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ —ç—Ç–∞–ø–∞ 26

| –ö—Ä–∏—Ç–µ—Ä–∏–π                  | –ë–∞–ª–ª—ã   | –û–ø–∏—Å–∞–Ω–∏–µ                              |
| ------------------------- | ------- | ------------------------------------- |
| **ML –º–æ–¥–µ–ª–∏**             | 60      | –ö–∞—á–µ—Å—Ç–≤–æ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π |
| **Feature Engineering**   | 40      | –ò–Ω–∂–µ–Ω–µ—Ä–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤                   |
| **MLOps –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞**  | 30      | –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è ML pipeline             |
| **Production Deployment** | 30      | –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ –ø—Ä–æ–¥–∞–∫—à–Ω |
| **–ò—Ç–æ–≥–æ**                 | **160** | –ú–∏–Ω–∏–º—É–º 112 –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –∫ —ç—Ç–∞–ø—É 27   |

### üéØ –°–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —ç—Ç–∞–ø–∞ 26 –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ **–≠—Ç–∞–ø—É 27: Research & Innovation**.
